# SPDX-License-Identifier: Apache-2.0

# Configuration for Cog ⚙️
# Reference: https://cog.run/yaml

# image name:version
image: "r8.im/ibm-granite/granite-4.0-h-small-fp8:0.1.0"

build:
  # Enable new Cog runtime implementation
  cog_runtime: true

  # set to true if your model requires a GPU
  gpu: true
  # This needs to match the CUDA version in pyproject.toml
  cuda: "12.8"

  # python version in the form '3.11' or '3.11.4'
  python_version: "3.12"

  # path to a Python requirements.txt file
  python_requirements: requirements.txt

  # commands run after the environment is setup
  # run:
    # Install FlashInfer for vllm
    #- --mount=type=cache,target=/root/.cache/pip pip install "flashinfer-python==0.2.6.post1" -i https://download.pytorch.org/whl/cu128/flashinfer

# predict.py defines how predictions are run on your model
predict: "predict.py:Predictor"

#concurrency capabilities of the model.
concurrency:
  max: 4
